### Overview
This C program analyzes the contents of a text file and displays:

* The total number of words 
* The number of unique words
* A list of the most frequently found words


### Command Line Arguments

* File name of text file to be analyzed.
* Optional: Integer value for the length of the "frequent words" list printed. If not specified, set to 10. If text file is so short that it does not contain at least the number of words specified, then the list is truncated to the appropriate length.

### Algorithm Description

The algorithm first uses a hash table to collect the word counts from a text file. There is a linked list structure at every index in the hash table, which is an implementation of the chaining strategy to cope with collisions. While the insertions into the hash table are taking place, another array collects pointers to every new Node struct that is being inserted. Once the hash table is finished being built it is no longer needed and the memory for it is freed. The array that was collecting pointers is then sorted using the qsort() function in stdlib.h. The summary output is then shown, and then the memory for the Nodes and the array holding the pointers to those Nodes are freed.

The hashing function counts the ASCII values of each character in the string but it gives more weight to the first, second, and third characters by multiplying them by 7, 5, and 3 respectively. As mentioned previously, the hash table contains linked list structures instead of the data directly to help cope with collisions.

The code could be improved by creating a better hash function to reduce the number of collisions. The characters I chose to split the input strings with could also potentially be improved because as it is "don" and "donâ€™t" are considered to be the same word, which is not accurate.